{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   tweet  sarcasm sentiment  \\\n0      \"Ø¯. #Ù…Ø­Ù…ÙˆØ¯_Ø§Ù„Ø¹Ù„Ø§ÙŠÙ„ÙŠ:Ø£Ø±Ù‰ Ø£Ù† Ø§Ù„ÙØ±ÙŠÙ‚ #Ø£Ø­Ù…Ø¯_Ø´ÙÙŠÙ‚ Ø±...    False       NEU   \n1      \"Ù…Ø¹ ÙÙŠØ¯Ø±Ø± ÙŠØ§ Ø¢Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± ğŸ˜ https://t.co/hrBeHb...    False       NEU   \n2      â€œØ§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†Ø› ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† ...     True       NEG   \n3      \"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...     True       NEG   \n4      \"Ù‚Ù„ Ø´Ø±Ù‚ Ø­Ù„Ø¨ ÙˆÙ„Ø§ ØªÙ‚Ù„ Ø­Ù„Ø¨ Ø§Ù„Ø´Ø±Ù‚ÙŠØ© ....ÙˆÙ‚Ù„ ØºØ±Ø¨ Ø­Ù„...    False       NEU   \n...                                                  ...      ...       ...   \n15543  ØªØ±Ø¨ÙƒÙ†ÙŠ Ø§Ù„Ø°ÙƒØ±Ù‰ Ù„ÙŠØ§ Ù…Ø± Ø·Ø§Ø±ÙŠÙ‡ÙˆØ§Ù†Ø³Ù‰ Ø§Ù„Ø¨Ø´Ø± Ø­ÙˆÙ„ÙŠ ÙˆØ§Ù„...    False       NEU   \n15544                 ÙˆØ§Ù†Ø§ Ø§Ø­Ø³Ø¨Ù‡Ù… Ø§Ù„Ø­ÙŠÙ† Ù…Ø§ÙŠØªØ±ÙƒÙˆÙ† Ø­Ø±ÙƒØ§ØªÙ‡Ù…    False       NEG   \n15545  #ÙÙ‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ø¯_ØºØ´Ø´Ø´Ø´Ø´Ø´Ø´Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ù…_Ø§Ù„Ø¨Ø¨Ø¨Ø¨Ø¨Ø¨ØµØµØµØµØµÙ…...    False       NEU   \n15546  - Ù„Ù€Ù€Ùˆ ÙƒÙ€Ù€Ø§Ù† Ø§Ù„Ø§Ù…Ù€Ù€Ù€Ø± Ø¨ÙŠØ¯ÙŠ Ù„Ø£Ø®ÙÙŠØª Ø§Ù†Ù‡ÙŠØ§Ø± Ø¯Ù…ÙˆØ¹ÙŠ...    False       NEG   \n15547  Ù Ù†ÙØ³Ø§Ø§Ø¡ Ø¨Ø§Ø¨Ù„ ÙƒÙ†Ù‘ ÙŠØ³Ù’Ø­Ø±Ù† Ø§Ù„Ø±Ù‘Ø¬Ù„ Ø¨Ø§Ù„Ø¹ÙŠÙÙˆÙ† ÙÙ‚Ø· ....     True       NEG   \n\n      dialect  \n0         msa  \n1         msa  \n2         msa  \n3        gulf  \n4         msa  \n...       ...  \n15543     msa  \n15544     msa  \n15545     msa  \n15546     msa  \n15547     msa  \n\n[15548 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>sarcasm</th>\n      <th>sentiment</th>\n      <th>dialect</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"Ø¯. #Ù…Ø­Ù…ÙˆØ¯_Ø§Ù„Ø¹Ù„Ø§ÙŠÙ„ÙŠ:Ø£Ø±Ù‰ Ø£Ù† Ø§Ù„ÙØ±ÙŠÙ‚ #Ø£Ø­Ù…Ø¯_Ø´ÙÙŠÙ‚ Ø±...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"Ù…Ø¹ ÙÙŠØ¯Ø±Ø± ÙŠØ§ Ø¢Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± ğŸ˜ https://t.co/hrBeHb...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>â€œØ§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†Ø› ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† ...</td>\n      <td>True</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"@ihe_94 @ya78m @amooo5 @badiajnikhar @Oukasaf...</td>\n      <td>True</td>\n      <td>NEG</td>\n      <td>gulf</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"Ù‚Ù„ Ø´Ø±Ù‚ Ø­Ù„Ø¨ ÙˆÙ„Ø§ ØªÙ‚Ù„ Ø­Ù„Ø¨ Ø§Ù„Ø´Ø±Ù‚ÙŠØ© ....ÙˆÙ‚Ù„ ØºØ±Ø¨ Ø­Ù„...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15543</th>\n      <td>ØªØ±Ø¨ÙƒÙ†ÙŠ Ø§Ù„Ø°ÙƒØ±Ù‰ Ù„ÙŠØ§ Ù…Ø± Ø·Ø§Ø±ÙŠÙ‡ÙˆØ§Ù†Ø³Ù‰ Ø§Ù„Ø¨Ø´Ø± Ø­ÙˆÙ„ÙŠ ÙˆØ§Ù„...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15544</th>\n      <td>ÙˆØ§Ù†Ø§ Ø§Ø­Ø³Ø¨Ù‡Ù… Ø§Ù„Ø­ÙŠÙ† Ù…Ø§ÙŠØªØ±ÙƒÙˆÙ† Ø­Ø±ÙƒØ§ØªÙ‡Ù…</td>\n      <td>False</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15545</th>\n      <td>#ÙÙ‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ø¯_ØºØ´Ø´Ø´Ø´Ø´Ø´Ø´Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ù…_Ø§Ù„Ø¨Ø¨Ø¨Ø¨Ø¨Ø¨ØµØµØµØµØµÙ…...</td>\n      <td>False</td>\n      <td>NEU</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15546</th>\n      <td>- Ù„Ù€Ù€Ùˆ ÙƒÙ€Ù€Ø§Ù† Ø§Ù„Ø§Ù…Ù€Ù€Ù€Ø± Ø¨ÙŠØ¯ÙŠ Ù„Ø£Ø®ÙÙŠØª Ø§Ù†Ù‡ÙŠØ§Ø± Ø¯Ù…ÙˆØ¹ÙŠ...</td>\n      <td>False</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n    <tr>\n      <th>15547</th>\n      <td>Ù Ù†ÙØ³Ø§Ø§Ø¡ Ø¨Ø§Ø¨Ù„ ÙƒÙ†Ù‘ ÙŠØ³Ù’Ø­Ø±Ù† Ø§Ù„Ø±Ù‘Ø¬Ù„ Ø¨Ø§Ù„Ø¹ÙŠÙÙˆÙ† ÙÙ‚Ø· ....</td>\n      <td>True</td>\n      <td>NEG</td>\n      <td>msa</td>\n    </tr>\n  </tbody>\n</table>\n<p>15548 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"training_data.csv\")\n",
    "test = pd.read_csv(\"testing_data.csv\")\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:14:20.821873600Z",
     "start_time": "2024-02-18T23:14:19.901469Z"
    }
   },
   "id": "9ee0bbb0626ce380"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet\n",
      "\"RT @M__albugaily: Ù‚Ø±Ø¯ÙˆØºØ§Ù† Ø­Ù„ÙŠÙ Ø§Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙŠØ¯Ø¹Ù… ØªÙ†Ø¸ÙŠÙ… Ø¯Ø§Ø¹Ø´ ÙÙŠ Ø³ÙˆØ±ÙŠØ§ ÙˆÙ…Ø¹ Ø°Ù„Ùƒ Ù„Ù… ÙŠØºØ¶Ø¨ Ø¹Ù„ÙŠÙ‡ Ø£Ø­Ø¯ ÙˆÙŠÙ‡Ø¯Ø¯Ù‡ ÙƒÙ…Ø§ ÙŠÙØ¹Ù„ Ù…Ø¹ Ù…ØµØ± Ø§Ù„ØªÙŠ ØªÙˆØ§Ø¬Ù‡ Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…â€¦\"    3\n",
      "\"RT @ahlalsunna2: #Ø³ÙˆØ±ÙŠØ§ :Ø§Ù„Ø¬ÙŠØ´ Ø§Ù„Ø³ÙˆØ±ÙŠ Ø§Ù„Ø­Ø± ÙŠØ³ÙŠØ·Ø± Ø¹Ù„Ù‰ Ù‚Ø±Ù‰ Ø¨Ø±Ø§ØºÙŠØ¯Ø© ÙˆØ§Ù„Ø´ÙŠØ® Ø±ÙŠØ­ ÙˆØ§Ù„Ø¨Ù„ ÙˆØ¬Ø§Ø±Ø² Ø¨Ø±ÙŠÙ Ø­Ù„Ø¨ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠ Ø¨Ø¹Ø¯ Ù…Ø¹Ø§Ø±Ùƒ Ù…Ø¹ ØªÙ†Ø¸ÙŠÙ… Ø¯Ø§Ø¹Ø´\"                3\n",
      "\"RT @ajmubasher: ÙˆØ²ÙŠØ± Ø®Ø§Ø±Ø¬ÙŠØ© #Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ§ ÙŠØ¯Ø¹Ùˆ Ù„Ù„ØªØ¸Ø§Ù‡Ø± Ø¶Ø¯ #Ø±ÙˆØ³ÙŠØ§#Ø­Ù„Ø¨ #Ø³ÙˆØ±ÙŠØ§https://t.co/0eBxBmcxua\"                                                  2\n",
      "\"RT @al8nas_ksa: Ù†Ø§Ø¦Ø¨ Ø§Ù„Ø¹Ø§Ù… Ù„Ø¬Ù…Ø§Ø¹Ø© #Ø§Ù„Ø¥Ø®ÙˆØ§Ù†_Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙÙŠ #Ø³ÙˆØ±ÙŠØ§ ..#Ø¥ÙŠØ±Ø§Ù† ØªØ±ÙŠØ¯ ØªØ³Ù„ÙŠÙ… Ø³ÙˆØ±ÙŠØ§ Ù„Ù„Ø¥Ø®ÙˆØ§Ù† ÙƒÙˆÙ†Ù‡Ù… Ø§Ù„Ø£Ù‚Ø±Ø¨ Ù„Ù‡Ù… . #ÙŠØ§_Ø¥Ø®ÙˆØ§Ù† #Ø£Ø±Ø¯ÙˆØºØ§Ù† hâ€¦\"        2\n",
      "Ø§Ù„ØµØ­Ø§ÙØ© Ø£Ø­Ø§Ø¯ÙŠØ« Ù…Ø¹ Ø£Ù†Ø§Ø³ ÙŠØ­Ø³Ù†ÙˆÙ† Ø§Ù„ÙƒÙ„Ø§Ù… Ù†Ù†Ø´Ø±Ù‡Ø§ Ø¹Ù„ÙŠ Ø£Ù†Ø§Ø³ Ù„Ø§ÙŠØ­Ø³Ù†ÙˆÙ† Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©. Ø£Ù†ÙŠØ³ Ù…Ù†ØµÙˆØ±                                                                 2\n",
      "                                                                                                                                                 ..\n",
      "ÙƒØ§Ù† Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø£Ù† ÙŠÙƒØªØ¨ÙˆØ§ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø­ÙˆØ§Ø¦Ø· Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©: Ø­Ø§ÙØ¸ÙˆØ§ Ø¹Ù„Ù‰ Ù†Ø¸Ø§ÙØ© Ù†ÙØ³ÙŠØªÙƒÙ…                                                                             1\n",
      "Ø¥Ù†Øª ÙØ§ÙƒØ± Ø¥Ù† Ù…ØªØ§Ø¨Ø¹ØªÙƒ Ù„Ø£ÙÙ„Ø§Ù…ÙŠ Ø£Ù‡Ù… Ù…Ù† Ø¨Ù„Ø¯ÙŠØŸ ÙÙŠ Ø³ØªÙŠÙ† Ø£Ù„Ù Ø¯Ø§Ù‡ÙŠØ© ÙˆØ¥ÙˆØ¹Ù‰ ØªØ±Ø¬Ø¹ ÙÙŠ ÙƒÙ„Ø§Ù…Ùƒ Ù‚Ø¨Ù„ Ù…Ø§ ØªØ¹ØªØ°Ø± Ø¹Ù„Ù‰ Ø¬Ù‡Ù„Ùƒ                                              1\n",
      "\"Ø¨Ø§Ù„Ø§Ø³Ø¹Ø§Ø±.. Ø¥Ø­ØµÙ„ÙŠ Ø¹Ù„Ù‰ #Ø¥Ø·Ù„Ø§Ù„Ø© #Ø¥Ù„ÙŠØ³Ø§ Ø§Ù„Ù…Ù…ÙŠÙ‘Ø²Ø©elissakh https://t.co/cTbfN3AmVc https://t.co/6LNNABTX0r\"                                            1\n",
      "\"Ø§Ù„ÙØ±ÙŠÙ‚ #Ø§Ø­Ù…Ø¯_Ø´ÙÙŠÙ‚ ØªÙ… Ø±ÙØ¹ Ø§Ø³Ù…Ù‡ Ù…Ù† Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ±Ù‚Ø¨ Ùˆ Ø§Ù„Ù…Ù†Ø¹ Ù…Ù† Ø§Ù„Ø³ÙØ± https://t.co/OdB0bGfjYQ\"                                                          1\n",
      "Ù Ù†ÙØ³Ø§Ø§Ø¡ Ø¨Ø§Ø¨Ù„ ÙƒÙ†Ù‘ ÙŠØ³Ù’Ø­Ø±Ù† Ø§Ù„Ø±Ù‘Ø¬Ù„ Ø¨Ø§Ù„Ø¹ÙŠÙÙˆÙ† ÙÙ‚Ø· .!! ÙˆÙ Ù†ÙØ³Ø§Ø§Ø¡Ù’ Ø§Ù„ÙŠÙÙˆÙ… ÙŠØ³Ù’Ø­ÙØ±Ù† Ø§Ù„Ø±Ù‘Ø¬Ù„ Ø¨Ø¬ÙØ³Ø¯Ù’ ÙƒØ§Ø§Ù…Ù„ .!!  îš˜                                             1\n",
      "Name: count, Length: 15481, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = data[\"tweet\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "data = pd.DataFrame(data, columns=[\"tweet\", \"sentiment\"])\n",
    "print(data[\"tweet\"].value_counts()) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:14:20.835182600Z",
     "start_time": "2024-02-18T23:14:20.553349700Z"
    }
   },
   "id": "583241c5c4e87882"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEG\n",
      "Ø§Ù Ù‡Ø°Ø§ğŸ¤¢\n"
     ]
    },
    {
     "data": {
      "text/plain": "'ÙÙŠÙ‡ Ù†Ø§Ø³ Ø¨ØªØ¨Ø¹ØªÙ„ÙŠ Ø±Ø³Ø§Ø¦Ù„ Ø¹ØªØ§Ø¨ Ø±Ù‚ÙŠÙ‚Ø© Ø¬Ø¯Ø§Ù‹ ÙˆØ§Ù„Ù„Ù‡ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ø© Ø£Ù†Ø§ Ù…Ø§ Ø¹Ù†Ø¯ÙŠØ´ Ø£ÙŠ Ù†ÙŠØ© Ø£ÙƒÙˆÙ† ÙƒØ§Ù‡Ù† ÙˆÙ„Ø§ Ù…Ù„Ø§Ùƒ ÙˆØ¨Ù‚ÙˆÙ„ Ù„Ù„Ø®Ø±ÙˆÙ ÙŠØ§ Ø®Ø±ÙˆÙ ÙÙŠ ÙˆØ´Ù‡ ÙˆÙ…Ø´ Ù‡ØªØºÙŠØ±'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['sentiment'][12]) # la classe qui est dans la ligne 12\n",
    "print(data['tweet'][12]) # le tweet qui est dans la ligne 12\n",
    "data[\"tweet\"][225] # le tweet qui est dans la ligne 225"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:14:21.222881100Z",
     "start_time": "2024-02-18T23:14:20.733651800Z"
    }
   },
   "id": "607638499e0e7ee"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'ÙÙŠÙ‡ Ù†Ø§Ø³ Ø¨ØªØ¨Ø¹ØªÙ„ÙŠ Ø±Ø³Ø§ÙŠÙ„ Ø¹ØªØ§Ø¨ Ø±Ù‚ÙŠÙ‚Ø© Ø¬Ø¯Ø§ ÙˆØ§Ù„Ù‡ ÙŠØ§ Ø¬Ù…Ø§Ø¹Ø© Ø§Ù†Ø§ Ù…Ø§ Ø¹Ù†Ø¯ÙŠØ´ Ø§ÙŠ Ù†ÙŠØ© Ø§ÙƒÙˆÙ† ÙƒØ§Ù‡Ù† ÙˆÙ„Ø§ Ù…Ù„Ø§Ùƒ ÙˆØ¨Ù‚ÙˆÙ„ Ù„Ø®Ø±ÙˆÙ ÙŠØ§ Ø®Ø±ÙˆÙ ÙÙŠ ÙˆØ´Ù‡ ÙˆÙ…Ø´ Ù‡ØªØºÙŠØ±'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from aaransia import transliterate, SourceLanguageError\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "for i in range(len(data)):\n",
    "    row = data.iloc[i][\"tweet\"]\n",
    "    try:\n",
    "        row = transliterate(row, source='ar', target='ar', universal=True)\n",
    "        data.loc[:, \"tweet\"][i] = row\n",
    "    except SourceLanguageError as e:\n",
    "        print(f\"Erreur de translittÃ©ration Ã  l'index {i}: {e}\")\n",
    "\n",
    "# Affichage d'un exemple aprÃ©s le premier prÃ©traitement\n",
    "data[\"tweet\"][225]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:16:59.696533400Z",
     "start_time": "2024-02-18T23:14:20.908000900Z"
    }
   },
   "id": "24ee327b40df9e89"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù Ù‡Ø°Ø§ğŸ¤¢\n"
     ]
    }
   ],
   "source": [
    " import re\n",
    "\n",
    "# Normalisation des donnÃ©es\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "    return text\n",
    "\n",
    "# Applique la normalisation aux tweets\n",
    "data['tweet'] = data['tweet'].apply(normalize_arabic)\n",
    "\n",
    "print(data['tweet'][12])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:17:05.355872500Z",
     "start_time": "2024-02-18T23:16:59.272183Z"
    }
   },
   "id": "8c2997a5434ed3af"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples avant faire le nettoyage :\n",
      "\"Ù…Ø¹ ÙÙŠØ¯Ø± ÙŠØ§ Ø§Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± ğŸ˜ htps:/t.co/hrBeHbkBNu\"\n",
      "â€œØ§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø§ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†Ø› ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† Ù„Ø§Ù„ØºØ§Ø¡ Ø§Ù„ØªØ³Ø¹ÙŠØ±Ù‡ ÙƒÙ„Ø§Ù‡Ù…Ø§ ÙŠØ±ÙŠØ¯ ØªØµÙÙŠÙ‡ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡ Ø¨Ø¬Ø¹Ù„Ù‡Ø§ Ø­Ø±Ù‡.â€ #Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø·\n",
      "\"@ihe_94 @ya78m @amoo5 @badiajnikhar @Oukasafa @reoshalm @Mnory202 Ù…Ø³Ø§ÙƒÙŠÙ† Ù…Ù† Ø§Ù„ØµØ¨Ø­ Ùˆ Ù‡ÙˆÙ…Ø§ Ø±Ø§ÙŠØ­ÙŠÙ† Ø±Ø§Ø¬Ø¹ÙŠÙ† Ø¹Ø§Ø§ ØºÙˆØºÙ„ ØªØ¹Ø¨Øª Ø¨Ø¯Ø§Ù„Ù‡Ù… Ù‡Ù‡Ù‡ ÙŠ\"\n",
      "Ø§Ù Ù‡Ø°Ø§ğŸ¤¢\n",
      "\n",
      "Exemples aprÃ¨s faire le nettoyage :\n",
      "Ù…Ø¹ ÙÙŠØ¯Ø± ÙŠØ§ Ø§Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø± \n",
      "Ø§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø§ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ† ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† Ù„Ø§Ù„ØºØ§Ø¡ Ø§Ù„ØªØ³Ø¹ÙŠØ±Ù‡ ÙƒÙ„Ø§Ù‡Ù…Ø§ ÙŠØ±ÙŠØ¯ ØªØµÙÙŠÙ‡ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡ Ø¨Ø¬Ø¹Ù„Ù‡Ø§ Ø­Ø±Ù‡ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø·\n",
      " Ù…Ø³Ø§ÙƒÙŠÙ† Ù…Ù† Ø§Ù„ØµØ¨Ø­ Ùˆ Ù‡ÙˆÙ…Ø§ Ø±Ø§ÙŠØ­ÙŠÙ† Ø±Ø§Ø¬Ø¹ÙŠÙ† Ø¹Ø§Ø§ ØºÙˆØºÙ„ ØªØ¹Ø¨Øª Ø¨Ø¯Ø§Ù„Ù‡Ù… Ù‡ ÙŠ\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(\"Exemples avant faire le nettoyage :\")\n",
    "print(data['tweet'][1])\n",
    "print(data['tweet'][2])\n",
    "print(data['tweet'][3])\n",
    "print(data['tweet'][12])\n",
    "\n",
    "# Applique les modifications\n",
    "\n",
    "# Supprimer les hyperliens\n",
    "data['tweet'] = [re.sub(r'http\\S+ | htps\\S+', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer les URL\n",
    "data['tweet'] = [re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer les mots commenÃ§ant par @\n",
    "data['tweet'] = [re.sub(r'@\\S+', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer # et _\n",
    "data['tweet'] = data['tweet'].str.replace(\"_\", \" \").str.replace(\"#\", \"\")\n",
    "\n",
    "data['tweet'] = data['tweet'].str.replace('. | , | ØŒ | Ø›', ' ')\n",
    "\n",
    "# Supprimer les mots rÃ©servÃ©s sur Twitter\n",
    "data['tweet'] = [re.sub(r'\\bRT\\b | \\bRetweeted\\b', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "data['tweet'] = [re.sub(r'[\\u0660-\\u0669\\u06F0-\\u06F9]+', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Supprimer les caractÃ¨res consÃ©cutifs en double\n",
    "data['tweet'] = [re.sub(r\"(.)\\1{2,}\", r\"\\1\", str(s)) for s in data['tweet']] if len(data['tweet']) > 2 else data['tweet'][:2]\n",
    "\n",
    "data['tweet'] = data['tweet'].str.replace('\\d+', ' ')\n",
    "data['tweet'] = data['tweet'].str.replace('\\n', ' ')\n",
    "data['tweet'] = data['tweet'].str.replace('/', ' ')\n",
    "data['tweet'] = [re.sub(r'[^\\w\\s]', '', str(s)) for s in data['tweet']]\n",
    "\n",
    "# Remplacer les valeurs nulles par une chaÃ®ne de caractÃ¨res vide\n",
    "data['tweet'] = data['tweet'].fillna('')\n",
    "\n",
    "# VÃ©rifie un exemple\n",
    "print(\"\\nExemples aprÃ¨s faire le nettoyage :\")\n",
    "print(data['tweet'][1])\n",
    "print(data['tweet'][2])\n",
    "print(data['tweet'][3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:17:05.394245900Z",
     "start_time": "2024-02-18T23:17:01.220660100Z"
    }
   },
   "id": "5d6c8e0a41d1676a"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¥Ø°', 'Ø¥Ø°Ø§', 'Ø¥Ø°Ù…Ø§', 'Ø¥Ø°Ù†', 'Ø£Ù', 'Ø£Ù‚Ù„', 'Ø£ÙƒØ«Ø±', 'Ø£Ù„Ø§', 'Ø¥Ù„Ø§', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø§Ù„Ù„ØªØ§Ù†', 'Ø§Ù„Ù„ØªÙŠØ§', 'Ø§Ù„Ù„ØªÙŠÙ†', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ', 'Ø¥Ù„Ù‰', 'Ø¥Ù„ÙŠÙƒ', 'Ø¥Ù„ÙŠÙƒÙ…', 'Ø¥Ù„ÙŠÙƒÙ…Ø§', 'Ø¥Ù„ÙŠÙƒÙ†', 'Ø£Ù…', 'Ø£Ù…Ø§', 'Ø£Ù…Ø§', 'Ø¥Ù…Ø§', 'Ø£Ù†', 'Ø¥Ù†', 'Ø¥Ù†Ø§', 'Ø£Ù†Ø§', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ…Ø§', 'Ø£Ù†ØªÙ†', 'Ø¥Ù†Ù…Ø§', 'Ø¥Ù†Ù‡', 'Ø£Ù†Ù‰', 'Ø£Ù†Ù‰', 'Ø¢Ù‡', 'Ø¢Ù‡Ø§', 'Ø£Ùˆ', 'Ø£ÙˆÙ„Ø§Ø¡', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø£ÙˆÙ‡', 'Ø¢ÙŠ', 'Ø£ÙŠ', 'Ø£ÙŠÙ‡Ø§', 'Ø¥ÙŠ', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†Ù…Ø§', 'Ø¥ÙŠÙ‡', 'Ø¨Ø®', 'Ø¨Ø³', 'Ø¨Ø¹Ø¯', 'Ø¨Ø¹Ø¶', 'Ø¨Ùƒ', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…Ø§', 'Ø¨ÙƒÙ†', 'Ø¨Ù„', 'Ø¨Ù„Ù‰', 'Ø¨Ù…Ø§', 'Ø¨Ù…Ø§Ø°Ø§', 'Ø¨Ù…Ù†', 'Ø¨Ù†Ø§', 'Ø¨Ù‡', 'Ø¨Ù‡Ø§', 'Ø¨Ù‡Ù…', 'Ø¨Ù‡Ù…Ø§', 'Ø¨Ù‡Ù†', 'Ø¨ÙŠ', 'Ø¨ÙŠÙ†', 'Ø¨ÙŠØ¯', 'ØªÙ„Ùƒ', 'ØªÙ„ÙƒÙ…', 'ØªÙ„ÙƒÙ…Ø§', 'ØªÙ‡', 'ØªÙŠ', 'ØªÙŠÙ†', 'ØªÙŠÙ†Ùƒ', 'Ø«Ù…', 'Ø«Ù…Ø©', 'Ø­Ø§Ø´Ø§', 'Ø­Ø¨Ø°Ø§', 'Ø­ØªÙ‰', 'Ø­ÙŠØ«', 'Ø­ÙŠØ«Ù…Ø§', 'Ø­ÙŠÙ†', 'Ø®Ù„Ø§', 'Ø¯ÙˆÙ†', 'Ø°Ø§', 'Ø°Ø§Øª', 'Ø°Ø§Ùƒ', 'Ø°Ø§Ù†', 'Ø°Ø§Ù†Ùƒ', 'Ø°Ù„Ùƒ', 'Ø°Ù„ÙƒÙ…', 'Ø°Ù„ÙƒÙ…Ø§', 'Ø°Ù„ÙƒÙ†', 'Ø°Ù‡', 'Ø°Ùˆ', 'Ø°ÙˆØ§', 'Ø°ÙˆØ§ØªØ§', 'Ø°ÙˆØ§ØªÙŠ', 'Ø°ÙŠ', 'Ø°ÙŠÙ†', 'Ø°ÙŠÙ†Ùƒ', 'Ø±ÙŠØ«', 'Ø³ÙˆÙ', 'Ø³ÙˆÙ‰', 'Ø´ØªØ§Ù†', 'Ø¹Ø¯Ø§', 'Ø¹Ø³Ù‰', 'Ø¹Ù„', 'Ø¹Ù„Ù‰', 'Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙ‡', 'Ø¹Ù…Ø§', 'Ø¹Ù†', 'Ø¹Ù†Ø¯', 'ØºÙŠØ±', 'ÙØ¥Ø°Ø§', 'ÙØ¥Ù†', 'ÙÙ„Ø§', 'ÙÙ…Ù†', 'ÙÙŠ', 'ÙÙŠÙ…', 'ÙÙŠÙ…Ø§', 'ÙÙŠÙ‡', 'ÙÙŠÙ‡Ø§', 'Ù‚Ø¯', 'ÙƒØ£Ù†', 'ÙƒØ£Ù†Ù…Ø§', 'ÙƒØ£ÙŠ', 'ÙƒØ£ÙŠÙ†', 'ÙƒØ°Ø§', 'ÙƒØ°Ù„Ùƒ', 'ÙƒÙ„', 'ÙƒÙ„Ø§', 'ÙƒÙ„Ø§Ù‡Ù…Ø§', 'ÙƒÙ„ØªØ§', 'ÙƒÙ„Ù…Ø§', 'ÙƒÙ„ÙŠÙƒÙ…Ø§', 'ÙƒÙ„ÙŠÙ‡Ù…Ø§', 'ÙƒÙ…', 'ÙƒÙ…', 'ÙƒÙ…Ø§', 'ÙƒÙŠ', 'ÙƒÙŠØª', 'ÙƒÙŠÙ', 'ÙƒÙŠÙÙ…Ø§', 'Ù„Ø§', 'Ù„Ø§Ø³ÙŠÙ…Ø§', 'Ù„Ø¯Ù‰', 'Ù„Ø³Øª', 'Ù„Ø³ØªÙ…', 'Ù„Ø³ØªÙ…Ø§', 'Ù„Ø³ØªÙ†', 'Ù„Ø³Ù†', 'Ù„Ø³Ù†Ø§', 'Ù„Ø¹Ù„', 'Ù„Ùƒ', 'Ù„ÙƒÙ…', 'Ù„ÙƒÙ…Ø§', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†Ù…Ø§', 'Ù„ÙƒÙŠ', 'Ù„ÙƒÙŠÙ„Ø§', 'Ù„Ù…', 'Ù„Ù…Ø§', 'Ù„Ù†', 'Ù„Ù†Ø§', 'Ù„Ù‡', 'Ù„Ù‡Ø§', 'Ù„Ù‡Ù…', 'Ù„Ù‡Ù…Ø§', 'Ù„Ù‡Ù†', 'Ù„Ùˆ', 'Ù„ÙˆÙ„Ø§', 'Ù„ÙˆÙ…Ø§', 'Ù„ÙŠ', 'Ù„Ø¦Ù†', 'Ù„ÙŠØª', 'Ù„ÙŠØ³', 'Ù„ÙŠØ³Ø§', 'Ù„ÙŠØ³Øª', 'Ù„ÙŠØ³ØªØ§', 'Ù„ÙŠØ³ÙˆØ§', 'Ù…Ø§', 'Ù…Ø§Ø°Ø§', 'Ù…ØªÙ‰', 'Ù…Ø°', 'Ù…Ø¹', 'Ù…Ù…Ø§', 'Ù…Ù…Ù†', 'Ù…Ù†', 'Ù…Ù†Ù‡', 'Ù…Ù†Ù‡Ø§', 'Ù…Ù†Ø°', 'Ù…Ù‡', 'Ù…Ù‡Ù…Ø§', 'Ù†Ø­Ù†', 'Ù†Ø­Ùˆ', 'Ù†Ø¹Ù…', 'Ù‡Ø§', 'Ù‡Ø§ØªØ§Ù†', 'Ù‡Ø§ØªÙ‡', 'Ù‡Ø§ØªÙŠ', 'Ù‡Ø§ØªÙŠÙ†', 'Ù‡Ø§Ùƒ', 'Ù‡Ø§Ù‡Ù†Ø§', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ø§Ù†', 'Ù‡Ø°Ù‡', 'Ù‡Ø°ÙŠ', 'Ù‡Ø°ÙŠÙ†', 'Ù‡ÙƒØ°Ø§', 'Ù‡Ù„', 'Ù‡Ù„Ø§', 'Ù‡Ù…', 'Ù‡Ù…Ø§', 'Ù‡Ù†', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ù‡Ù†Ø§Ù„Ùƒ', 'Ù‡Ùˆ', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ù‡ÙŠ', 'Ù‡ÙŠØ§', 'Ù‡ÙŠØª', 'Ù‡ÙŠÙ‡Ø§Øª', 'ÙˆØ§Ù„Ø°ÙŠ', 'ÙˆØ§Ù„Ø°ÙŠÙ†', 'ÙˆØ¥Ø°', 'ÙˆØ¥Ø°Ø§', 'ÙˆØ¥Ù†', 'ÙˆÙ„Ø§', 'ÙˆÙ„ÙƒÙ†', 'ÙˆÙ„Ùˆ', 'ÙˆÙ…Ø§', 'ÙˆÙ…Ù†', 'ÙˆÙ‡Ùˆ', 'ÙŠØ§', 'Ø£Ø¨ÙŒ', 'Ø£Ø®ÙŒ', 'Ø­Ù…ÙŒ', 'ÙÙˆ', 'Ø£Ù†ØªÙ', 'ÙŠÙ†Ø§ÙŠØ±', 'ÙØ¨Ø±Ø§ÙŠØ±', 'Ù…Ø§Ø±Ø³', 'Ø£Ø¨Ø±ÙŠÙ„', 'Ù…Ø§ÙŠÙˆ', 'ÙŠÙˆÙ†ÙŠÙˆ', 'ÙŠÙˆÙ„ÙŠÙˆ', 'Ø£ØºØ³Ø·Ø³', 'Ø³Ø¨ØªÙ…Ø¨Ø±', 'Ø£ÙƒØªÙˆØ¨Ø±', 'Ù†ÙˆÙÙ…Ø¨Ø±', 'Ø¯ÙŠØ³Ù…Ø¨Ø±', 'Ø¬Ø§Ù†ÙÙŠ', 'ÙÙŠÙØ±ÙŠ', 'Ù…Ø§Ø±Ø³', 'Ø£ÙØ±ÙŠÙ„', 'Ù…Ø§ÙŠ', 'Ø¬ÙˆØ§Ù†', 'Ø¬ÙˆÙŠÙ„ÙŠØ©', 'Ø£ÙˆØª', 'ÙƒØ§Ù†ÙˆÙ†', 'Ø´Ø¨Ø§Ø·', 'Ø¢Ø°Ø§Ø±', 'Ù†ÙŠØ³Ø§Ù†', 'Ø£ÙŠØ§Ø±', 'Ø­Ø²ÙŠØ±Ø§Ù†', 'ØªÙ…ÙˆØ²', 'Ø¢Ø¨', 'Ø£ÙŠÙ„ÙˆÙ„', 'ØªØ´Ø±ÙŠÙ†', 'Ø¯ÙˆÙ„Ø§Ø±', 'Ø¯ÙŠÙ†Ø§Ø±', 'Ø±ÙŠØ§Ù„', 'Ø¯Ø±Ù‡Ù…', 'Ù„ÙŠØ±Ø©', 'Ø¬Ù†ÙŠÙ‡', 'Ù‚Ø±Ø´', 'Ù…Ù„ÙŠÙ…', 'ÙÙ„Ø³', 'Ù‡Ù„Ù„Ø©', 'Ø³Ù†ØªÙŠÙ…', 'ÙŠÙˆØ±Ùˆ', 'ÙŠÙ†', 'ÙŠÙˆØ§Ù†', 'Ø´ÙŠÙƒÙ„', 'ÙˆØ§Ø­Ø¯', 'Ø§Ø«Ù†Ø§Ù†', 'Ø«Ù„Ø§Ø«Ø©', 'Ø£Ø±Ø¨Ø¹Ø©', 'Ø®Ù…Ø³Ø©', 'Ø³ØªØ©', 'Ø³Ø¨Ø¹Ø©', 'Ø«Ù…Ø§Ù†ÙŠØ©', 'ØªØ³Ø¹Ø©', 'Ø¹Ø´Ø±Ø©', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†Ø§', 'Ø§Ø«Ù†ÙŠ', 'Ø¥Ø­Ø¯Ù‰', 'Ø«Ù„Ø§Ø«', 'Ø£Ø±Ø¨Ø¹', 'Ø®Ù…Ø³', 'Ø³Øª', 'Ø³Ø¨Ø¹', 'Ø«Ù…Ø§Ù†ÙŠ', 'ØªØ³Ø¹', 'Ø¹Ø´Ø±', 'Ø«Ù…Ø§Ù†', 'Ø³Ø¨Øª', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†ÙŠÙ†', 'Ø«Ù„Ø§Ø«Ø§Ø¡', 'Ø£Ø±Ø¨Ø¹Ø§Ø¡', 'Ø®Ù…ÙŠØ³', 'Ø¬Ù…Ø¹Ø©', 'Ø£ÙˆÙ„', 'Ø«Ø§Ù†', 'Ø«Ø§Ù†ÙŠ', 'Ø«Ø§Ù„Ø«', 'Ø±Ø§Ø¨Ø¹', 'Ø®Ø§Ù…Ø³', 'Ø³Ø§Ø¯Ø³', 'Ø³Ø§Ø¨Ø¹', 'Ø«Ø§Ù…Ù†', 'ØªØ§Ø³Ø¹', 'Ø¹Ø§Ø´Ø±', 'Ø­Ø§Ø¯ÙŠ', 'Ø£', 'Ø¨', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'Ø¡', 'Ù‰', 'Ø¢', 'Ø¤', 'Ø¦', 'Ø£', 'Ø©', 'Ø£Ù„Ù', 'Ø¨Ø§Ø¡', 'ØªØ§Ø¡', 'Ø«Ø§Ø¡', 'Ø¬ÙŠÙ…', 'Ø­Ø§Ø¡', 'Ø®Ø§Ø¡', 'Ø¯Ø§Ù„', 'Ø°Ø§Ù„', 'Ø±Ø§Ø¡', 'Ø²Ø§ÙŠ', 'Ø³ÙŠÙ†', 'Ø´ÙŠÙ†', 'ØµØ§Ø¯', 'Ø¶Ø§Ø¯', 'Ø·Ø§Ø¡', 'Ø¸Ø§Ø¡', 'Ø¹ÙŠÙ†', 'ØºÙŠÙ†', 'ÙØ§Ø¡', 'Ù‚Ø§Ù', 'ÙƒØ§Ù', 'Ù„Ø§Ù…', 'Ù…ÙŠÙ…', 'Ù†ÙˆÙ†', 'Ù‡Ø§Ø¡', 'ÙˆØ§Ùˆ', 'ÙŠØ§Ø¡', 'Ù‡Ù…Ø²Ø©', 'ÙŠ', 'Ù†Ø§', 'Ùƒ', 'ÙƒÙ†', 'Ù‡', 'Ø¥ÙŠØ§Ù‡', 'Ø¥ÙŠØ§Ù‡Ø§', 'Ø¥ÙŠØ§Ù‡Ù…Ø§', 'Ø¥ÙŠØ§Ù‡Ù…', 'Ø¥ÙŠØ§Ù‡Ù†', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ…Ø§', 'Ø¥ÙŠØ§ÙƒÙ…', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ†', 'Ø¥ÙŠØ§ÙŠ', 'Ø¥ÙŠØ§Ù†Ø§', 'Ø£ÙˆÙ„Ø§Ù„Ùƒ', 'ØªØ§Ù†Ù', 'ØªØ§Ù†ÙÙƒ', 'ØªÙÙ‡', 'ØªÙÙŠ', 'ØªÙÙŠÙ’Ù†Ù', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø°Ø§Ù†Ù', 'Ø°ÙÙ‡', 'Ø°ÙÙŠ', 'Ø°ÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ¤Ù„Ø§Ø¡', 'Ù‡ÙØ§ØªØ§Ù†Ù', 'Ù‡ÙØ§ØªÙÙ‡', 'Ù‡ÙØ§ØªÙÙŠ', 'Ù‡ÙØ§ØªÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ°Ø§', 'Ù‡ÙØ°Ø§Ù†Ù', 'Ù‡ÙØ°ÙÙ‡', 'Ù‡ÙØ°ÙÙŠ', 'Ù‡ÙØ°ÙÙŠÙ’Ù†Ù', 'Ø§Ù„Ø£Ù„Ù‰', 'Ø§Ù„Ø£Ù„Ø§Ø¡', 'Ø£Ù„', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø°ÙŠØª', 'ÙƒØ£ÙŠÙ‘', 'ÙƒØ£ÙŠÙ‘Ù†', 'Ø¨Ø¶Ø¹', 'ÙÙ„Ø§Ù†', 'ÙˆØ§', 'Ø¢Ù…ÙŠÙ†Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ø§Ù‹', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙ‘', 'Ø£Ù…Ø§Ù…Ùƒ', 'Ø£Ù…Ø§Ù…ÙƒÙ', 'Ø£ÙˆÙ‘Ù‡Ù’', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ†Ù‘', 'Ø¥ÙŠÙ‡Ù', 'Ø¨Ø®Ù', 'Ø¨Ø³Ù‘', 'Ø¨ÙØ³Ù’', 'Ø¨Ø·Ø¢Ù†', 'Ø¨ÙÙ„Ù’Ù‡Ù', 'Ø­Ø§ÙŠ', 'Ø­ÙØ°Ø§Ø±Ù', 'Ø­ÙŠÙÙ‘', 'Ø­ÙŠÙÙ‘', 'Ø¯ÙˆÙ†Ùƒ', 'Ø±ÙˆÙŠØ¯Ùƒ', 'Ø³Ø±Ø¹Ø§Ù†', 'Ø´ØªØ§Ù†Ù', 'Ø´ÙØªÙÙ‘Ø§Ù†Ù', 'ØµÙ‡Ù’', 'ØµÙ‡Ù', 'Ø·Ø§Ù‚', 'Ø·ÙÙ‚', 'Ø¹ÙØ¯ÙØ³Ù’', 'ÙƒÙØ®', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙƒÙ…', 'Ù…ÙƒØ§Ù†ÙƒÙ…Ø§', 'Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘', 'Ù†ÙØ®Ù’', 'Ù‡Ø§ÙƒÙ', 'Ù‡ÙØ¬Ù’', 'Ù‡Ù„Ù…', 'Ù‡ÙŠÙ‘Ø§', 'Ù‡ÙÙŠÙ’Ù‡Ø§Øª', 'ÙˆØ§', 'ÙˆØ§Ù‡Ø§Ù‹', 'ÙˆØ±Ø§Ø¡ÙÙƒ', 'ÙˆÙØ´Ù’ÙƒÙØ§Ù†Ù', 'ÙˆÙÙŠÙ’', 'ÙŠÙØ¹Ù„Ø§Ù†', 'ØªÙØ¹Ù„Ø§Ù†', 'ÙŠÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙŠÙ†', 'Ø§ØªØ®Ø°', 'Ø£Ù„ÙÙ‰', 'ØªØ®Ø°', 'ØªØ±Ùƒ', 'ØªØ¹Ù„ÙÙ‘Ù…', 'Ø¬Ø¹Ù„', 'Ø­Ø¬Ø§', 'Ø­Ø¨ÙŠØ¨', 'Ø®Ø§Ù„', 'Ø­Ø³Ø¨', 'Ø®Ø§Ù„', 'Ø¯Ø±Ù‰', 'Ø±Ø£Ù‰', 'Ø²Ø¹Ù…', 'ØµØ¨Ø±', 'Ø¸Ù†ÙÙ‘', 'Ø¹Ø¯ÙÙ‘', 'Ø¹Ù„Ù…', 'ØºØ§Ø¯Ø±', 'Ø°Ù‡Ø¨', 'ÙˆØ¬Ø¯', 'ÙˆØ±Ø¯', 'ÙˆÙ‡Ø¨', 'Ø£Ø³ÙƒÙ†', 'Ø£Ø·Ø¹Ù…', 'Ø£Ø¹Ø·Ù‰', 'Ø±Ø²Ù‚', 'Ø²ÙˆØ¯', 'Ø³Ù‚Ù‰', 'ÙƒØ³Ø§', 'Ø£Ø®Ø¨Ø±', 'Ø£Ø±Ù‰', 'Ø£Ø¹Ù„Ù…', 'Ø£Ù†Ø¨Ø£', 'Ø­Ø¯ÙØ«', 'Ø®Ø¨ÙÙ‘Ø±', 'Ù†Ø¨ÙÙ‘Ø§', 'Ø£ÙØ¹Ù„ Ø¨Ù‡', 'Ù…Ø§ Ø£ÙØ¹Ù„Ù‡', 'Ø¨Ø¦Ø³', 'Ø³Ø§Ø¡', 'Ø·Ø§Ù„Ù…Ø§', 'Ù‚Ù„Ù…Ø§', 'Ù„Ø§Øª', 'Ù„ÙƒÙ†ÙÙ‘', 'Ø¡Ù', 'Ø£Ø¬Ù„', 'Ø¥Ø°Ø§Ù‹', 'Ø£Ù…Ù‘Ø§', 'Ø¥Ù…Ù‘Ø§', 'Ø¥Ù†ÙÙ‘', 'Ø£Ù†Ù‹Ù‘', 'Ø£Ù‰', 'Ø¥Ù‰', 'Ø£ÙŠØ§', 'Ø¨', 'Ø«Ù…ÙÙ‘', 'Ø¬Ù„Ù„', 'Ø¬ÙŠØ±', 'Ø±ÙØ¨ÙÙ‘', 'Ø³', 'Ø¹Ù„Ù‹Ù‘', 'Ù', 'ÙƒØ£Ù†Ù‘', 'ÙƒÙ„ÙÙ‘Ø§', 'ÙƒÙ‰', 'Ù„', 'Ù„Ø§Øª', 'Ù„Ø¹Ù„ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù…', 'Ù†ÙÙ‘', 'Ù‡Ù„Ù‘Ø§', 'ÙˆØ§', 'Ø£Ù„', 'Ø¥Ù„Ù‘Ø§', 'Øª', 'Ùƒ', 'Ù„Ù…Ù‘Ø§', 'Ù†', 'Ù‡', 'Ùˆ', 'Ø§', 'ÙŠ', 'ØªØ¬Ø§Ù‡', 'ØªÙ„Ù‚Ø§Ø¡', 'Ø¬Ù…ÙŠØ¹', 'Ø­Ø³Ø¨', 'Ø³Ø¨Ø­Ø§Ù†', 'Ø´Ø¨Ù‡', 'Ù„Ø¹Ù…Ø±', 'Ù…Ø«Ù„', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ùˆ', 'Ø£Ø®Ùˆ', 'Ø­Ù…Ùˆ', 'ÙÙˆ', 'Ù…Ø¦Ø©', 'Ù…Ø¦ØªØ§Ù†', 'Ø«Ù„Ø§Ø«Ù…Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø¦Ø©', 'Ø³ØªÙ…Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø¦Ø©', 'Ø«Ù…Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø¦Ø©', 'Ù…Ø§Ø¦Ø©', 'Ø«Ù„Ø§Ø«Ù…Ø§Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø§Ø¦Ø©', 'Ø³ØªÙ…Ø§Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø«Ù…Ø§Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø§Ø¦Ø©', 'Ø¹Ø´Ø±ÙˆÙ†', 'Ø«Ù„Ø§Ø«ÙˆÙ†', 'Ø§Ø±Ø¨Ø¹ÙˆÙ†', 'Ø®Ù…Ø³ÙˆÙ†', 'Ø³ØªÙˆÙ†', 'Ø³Ø¨Ø¹ÙˆÙ†', 'Ø«Ù…Ø§Ù†ÙˆÙ†', 'ØªØ³Ø¹ÙˆÙ†', 'Ø¹Ø´Ø±ÙŠÙ†', 'Ø«Ù„Ø§Ø«ÙŠÙ†', 'Ø§Ø±Ø¨Ø¹ÙŠÙ†', 'Ø®Ù…Ø³ÙŠÙ†', 'Ø³ØªÙŠÙ†', 'Ø³Ø¨Ø¹ÙŠÙ†', 'Ø«Ù…Ø§Ù†ÙŠÙ†', 'ØªØ³Ø¹ÙŠÙ†', 'Ø¨Ø¶Ø¹', 'Ù†ÙŠÙ', 'Ø£Ø¬Ù…Ø¹', 'Ø¬Ù…ÙŠØ¹', 'Ø¹Ø§Ù…Ø©', 'Ø¹ÙŠÙ†', 'Ù†ÙØ³', 'Ù„Ø§ Ø³ÙŠÙ…Ø§', 'Ø£ØµÙ„Ø§', 'Ø£Ù‡Ù„Ø§', 'Ø£ÙŠØ¶Ø§', 'Ø¨Ø¤Ø³Ø§', 'Ø¨Ø¹Ø¯Ø§', 'Ø¨ØºØªØ©', 'ØªØ¹Ø³Ø§', 'Ø­Ù‚Ø§', 'Ø­Ù…Ø¯Ø§', 'Ø®Ù„Ø§ÙØ§', 'Ø®Ø§ØµØ©', 'Ø¯ÙˆØ§Ù„ÙŠÙƒ', 'Ø³Ø­Ù‚Ø§', 'Ø³Ø±Ø§', 'Ø³Ù…Ø¹Ø§', 'ØµØ¨Ø±Ø§', 'ØµØ¯Ù‚Ø§', 'ØµØ±Ø§Ø­Ø©', 'Ø·Ø±Ø§', 'Ø¹Ø¬Ø¨Ø§', 'Ø¹ÙŠØ§Ù†Ø§', 'ØºØ§Ù„Ø¨Ø§', 'ÙØ±Ø§Ø¯Ù‰', 'ÙØ¶Ù„Ø§', 'Ù‚Ø§Ø·Ø¨Ø©', 'ÙƒØ«ÙŠØ±Ø§', 'Ù„Ø¨ÙŠÙƒ', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ø¯Ø§', 'Ø¥Ø²Ø§Ø¡', 'Ø£ØµÙ„Ø§', 'Ø§Ù„Ø¢Ù†', 'Ø£Ù…Ø¯', 'Ø£Ù…Ø³', 'Ø¢Ù†ÙØ§', 'Ø¢Ù†Ø§Ø¡', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙˆÙ„', 'Ø£ÙŠÙ‘Ø§Ù†', 'ØªØ§Ø±Ø©', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø­Ù‚Ø§', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'Ø¶Ø­ÙˆØ©', 'Ø¹ÙˆØ¶', 'ØºØ¯Ø§', 'ØºØ¯Ø§Ø©', 'Ù‚Ø·Ù‘', 'ÙƒÙ„Ù‘Ù…Ø§', 'Ù„Ø¯Ù†', 'Ù„Ù…Ù‘Ø§', 'Ù…Ø±Ù‘Ø©', 'Ù‚Ø¨Ù„', 'Ø®Ù„Ù', 'Ø£Ù…Ø§Ù…', 'ÙÙˆÙ‚', 'ØªØ­Øª', 'ÙŠÙ…ÙŠÙ†', 'Ø´Ù…Ø§Ù„', 'Ø§Ø±ØªØ¯Ù‘', 'Ø§Ø³ØªØ­Ø§Ù„', 'Ø£ØµØ¨Ø­', 'Ø£Ø¶Ø­Ù‰', 'Ø¢Ø¶', 'Ø£Ù…Ø³Ù‰', 'Ø§Ù†Ù‚Ù„Ø¨', 'Ø¨Ø§Øª', 'ØªØ¨Ø¯Ù‘Ù„', 'ØªØ­ÙˆÙ‘Ù„', 'Ø­Ø§Ø±', 'Ø±Ø¬Ø¹', 'Ø±Ø§Ø­', 'ØµØ§Ø±', 'Ø¸Ù„Ù‘', 'Ø¹Ø§Ø¯', 'ØºØ¯Ø§', 'ÙƒØ§Ù†', 'Ù…Ø§ Ø§Ù†ÙÙƒ', 'Ù…Ø§ Ø¨Ø±Ø­', 'Ù…Ø§Ø¯Ø§Ù…', 'Ù…Ø§Ø²Ø§Ù„', 'Ù…Ø§ÙØªØ¦', 'Ø§Ø¨ØªØ¯Ø£', 'Ø£Ø®Ø°', 'Ø§Ø®Ù„ÙˆÙ„Ù‚', 'Ø£Ù‚Ø¨Ù„', 'Ø§Ù†Ø¨Ø±Ù‰', 'Ø£Ù†Ø´Ø£', 'Ø£ÙˆØ´Ùƒ', 'Ø¬Ø¹Ù„', 'Ø­Ø±Ù‰', 'Ø´Ø±Ø¹', 'Ø·ÙÙ‚', 'Ø¹Ù„Ù‚', 'Ù‚Ø§Ù…', 'ÙƒØ±Ø¨', 'ÙƒØ§Ø¯', 'Ù‡Ø¨Ù‘']\n",
      "\n",
      "Exemples aprÃ©s faire le nettoyage des stop-words:\n",
      "ÙÙŠØ¯Ø± Ø§Ø¬Ø§ ÙˆØ§Ù„ÙƒØ¨Ø§Ø±\n",
      "Ø§Ù„Ø¯Ø§Ø¹ÙˆÙ† Ù„Ù…Ø¨Ø¯Ø§ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø· Ø§Ù„Ø¬Ù†Ø³ÙŠÙ† ÙƒØ§Ù„Ø¯Ø§Ø¹ÙŠÙ† Ù„Ø§Ù„ØºØ§Ø¡ Ø§Ù„ØªØ³Ø¹ÙŠØ±Ù‡ ÙŠØ±ÙŠØ¯ ØªØµÙÙŠÙ‡ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡ Ø¨Ø¬Ø¹Ù„Ù‡Ø§ Ø­Ø±Ù‡ Ø§Ù„Ø§Ø®ØªÙ„Ø§Ø·\n",
      "Ù…Ø³Ø§ÙƒÙŠÙ† Ø§Ù„ØµØ¨Ø­ Ù‡ÙˆÙ…Ø§ Ø±Ø§ÙŠØ­ÙŠÙ† Ø±Ø§Ø¬Ø¹ÙŠÙ† Ø¹Ø§Ø§ ØºÙˆØºÙ„ ØªØ¹Ø¨Øª Ø¨Ø¯Ø§Ù„Ù‡Ù…\n",
      "Ø§Ù\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Affiche quelques mots (stop-words)\n",
    "print(stopwords.words('arabic'))\n",
    "\n",
    "stopwords = list(set(nltk.corpus.stopwords.words('arabic')))\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "# Affichage un exemple apÃ©s faire le nettoyage des donnÃ©es (stop-words)\n",
    "print(\"\\nExemples aprÃ©s faire le nettoyage des stop-words:\")\n",
    "print(data['tweet'][1])\n",
    "print(data['tweet'][2])\n",
    "print(data['tweet'][3])\n",
    "print(data['tweet'][12])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:17:27.037772300Z",
     "start_time": "2024-02-18T23:17:05.260971100Z"
    }
   },
   "id": "b42a916450cb221d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù\n"
     ]
    }
   ],
   "source": [
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "# Stemmer_LIGHT : Supprimer \"suffixes\" et \"affixes\" \n",
    "ArListem = ArabicLightStemmer()\n",
    "def stemmer_light(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "# Root Stemming : Transformer le mot dans sa forme racine\n",
    "def stemmer_root(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "sentences = [stemmer_light(text) for text in data['tweet']]\n",
    "\n",
    "data['tweet'] = sentences\n",
    "print(data['tweet'][12])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:19.381450Z",
     "start_time": "2024-02-18T23:17:27.015716100Z"
    }
   },
   "id": "202978e5e4add8e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ø¹Ù„', 2710),\n",
      " ('Ø§Ù†', 2470),\n",
      " ('Ù„Ø§', 1311),\n",
      " ('Ù„Ù‡', 1291),\n",
      " ('Ù…ØµØ±', 1098),\n",
      " ('tco', 1067),\n",
      " ('Ù„ÙŠ', 1032),\n",
      " ('Ù†Ø§', 853),\n",
      " ('Ù†Øª', 799),\n",
      " ('Ø¹Ù„Ù‰', 757),\n",
      " ('Ø­Ø¯', 644),\n",
      " ('Ø­Ù„Ø¨', 599),\n",
      " ('ÙØ§', 577),\n",
      " ('ÙƒÙ„', 553),\n",
      " ('Ù‚ÙˆÙ„', 547),\n",
      " ('Ù…Ø´', 541),\n",
      " ('Ø±ÙŠÙŠØ³', 527),\n",
      " ('Ø¯ÙˆÙ„', 526),\n",
      " ('Ø§Ùˆ', 483),\n",
      " ('Ù†Ù‡', 469)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in  vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Nous inspectons le mot principal dans un Ã©chantillon de notre corpus sans dÃ©river les mots\n",
    "from pprint import pprint\n",
    "\n",
    "topWords=get_top_n_words(data['tweet'],n=20)\n",
    "pprint(topWords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.235798500Z",
     "start_time": "2024-02-18T23:19:19.034426800Z"
    }
   },
   "id": "c283a8e07bc214cc"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arabic_reshaper'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01marabic_reshaper\u001B[39;00m\n\u001B[0;32m      4\u001B[0m arab_stopwords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(nltk\u001B[38;5;241m.\u001B[39mcorpus\u001B[38;5;241m.\u001B[39mstopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marabic\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[0;32m      5\u001B[0m sample_corpus\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'arabic_reshaper'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import arabic_reshaper\n",
    "\n",
    "arab_stopwords = list(set(nltk.corpus.stopwords.words(\"arabic\")))\n",
    "sample_corpus=' '.join(data['tweet'])\n",
    "data_arb = arabic_reshaper.reshape(sample_corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:37:49.773997700Z",
     "start_time": "2024-02-18T23:37:49.695534Z"
    }
   },
   "id": "e6c08a1dd619d489"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_datasets= []\n",
    "\n",
    "print(data[\"sentiment\"].value_counts())\n",
    "sentiment = list(data[\"sentiment\"].unique())\n",
    "print(sentiment)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data[\"tweet\"])\n",
    "vocab_size = len(vectorizer.vocabulary_)\n",
    "print(f\"Il y a {vocab_size} mots diffÃ©rents dans notre corpus.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.510610400Z"
    }
   },
   "id": "82c21ac355f91e02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenizedWords = []\n",
    "documents = []\n",
    "\n",
    "# Chaque document contient tuple ==> la liste de mot et categorie\n",
    "for i in data.index:\n",
    "    sentiment = data[\"sentiment\"][i]\n",
    "    review = data[\"tweet\"][i]\n",
    "    tokenizedWord = word_tokenize(review)\n",
    "    document = [tokenizedWord, sentiment]\n",
    "    documents.append(document)\n",
    "\n",
    "# Chaque element de documents contient une liste = [contient tous les mots de document, son categorie]\n",
    "sentiment\n",
    "print(\"Taille documents : \", len(documents))\n",
    "data[\"tweet\"][4]\n",
    "# Chaque Ã©lÃ©ment de documents contient une liste qui contient tous les mots prÃ©sents dans ce document et son catÃ©gorie (sentiment soit 0 ou bien 1) comme par exemple ce commentaire \n",
    "documents[4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.520108300Z"
    }
   },
   "id": "5dbeb87e8aad7697"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data['tweet'][12])\n",
    "\n",
    "listeall=[]\n",
    "\n",
    "for i in data[\"tweet\"]:\n",
    "    review = i\n",
    "    tokenizedWord = word_tokenize(review)\n",
    "    for j in tokenizedWord:\n",
    "        listeall.append(j)\n",
    "# listeall est une liste qui contient tous les mots de tous les documents , qu'on va utiliser par la suite dans la partie de bag of word \n",
    "print(len(listeall))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.529228800Z"
    }
   },
   "id": "bb4fe5f8418da2d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in listeall)\n",
    "word_features = list(all_words)[:2000]\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "df_fdist = pd.DataFrame.from_dict(all_words, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "print(df_fdist)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.794200100Z",
     "start_time": "2024-02-18T23:19:25.559637100Z"
    }
   },
   "id": "971d3a7bd7518976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "\n",
    "# Former le classificateur Naive Bayes\n",
    "model = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Tester le modÃ¨le\n",
    "print(\"Accuracy avec le model Naive Bayes : \", nltk.classify.accuracy(model, test_set), \"%\")\n",
    "test_set[0][0]\n",
    "model.classify(test_set[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.576891200Z"
    }
   },
   "id": "d58a0dd5ce38c696"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_result = []\n",
    "gold_result = []\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    test_result.append(model.classify(test_set[i][0]))\n",
    "    gold_result.append(test_set[i][1])\n",
    "\n",
    "CM = nltk.ConfusionMatrix(gold_result, test_result)\n",
    "print(CM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.599693900Z"
    }
   },
   "id": "4be43b2ab74f44f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.show_most_informative_features(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.638610100Z"
    }
   },
   "id": "8d0b64bd3e36232c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Vectorisation des donnÃ©es textuelles\n",
    "vectorizer = CountVectorizer(max_features = 8000)\n",
    "X = data[\"tweet\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "# SÃ©paration des donnÃ©es en 2 parties : entraÃ®nement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "modelMulti = MultinomialNB()\n",
    "modelMulti.fit(train_X, train_y)\n",
    "\n",
    "# Test du classificateur\n",
    "predictions = modelMulti.predict(test_X)\n",
    "\n",
    "# Calcul des mÃ©triques de performance\n",
    "score_NVB = accuracy_score(test_y, predictions) * 100\n",
    "precision_NVB = precision_score(test_y, predictions, average = None) * 100\n",
    "recall_NVB = recall_score(test_y, predictions, average = None) * 100\n",
    "\n",
    "# Affichage des rÃ©sultats\n",
    "print('Accuracy du modÃ¨le sur l\\'ensemble de test : ', round(score_NVB, 2), '%')\n",
    "report = classification_report(test_y, predictions)\n",
    "print(report) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.684793500Z"
    }
   },
   "id": "d0d6397b40cb14af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for N in range(1,10):\n",
    "    vectorizer_G = CountVectorizer(max_features=8000, ngram_range=(1,N))\n",
    "\n",
    "    X = data[\"tweet\"]\n",
    "    y = data[\"sentiment\"]\n",
    "\n",
    "    # SÃ©paration des donnÃ©es en 2 parties : entraÃ®nement / test\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "    train_X = vectorizer_G.fit_transform(train_X)\n",
    "    test_X = vectorizer_G.transform(test_X)\n",
    "    \n",
    "    # EntraÃ®nement du modÃ¨le\n",
    "    modelMulti_G = MultinomialNB()\n",
    "    modelMulti_G.fit(train_X, train_y)\n",
    "    \n",
    "    # Test du classificateur\n",
    "    predictions_2G = modelMulti_G.predict(test_X)\n",
    "    print('Accuracy du modÃ¨le avec {} gramme: '.format(N), round(accuracy_score(test_y, predictions_2G) * 100, 2), '%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.695306600Z"
    }
   },
   "id": "2500bae76258f607"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "vectorizer_G = CountVectorizer(max_features=8000, ngram_range=(1,3))\n",
    "\n",
    "X = data[\"tweet\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "# SÃ©paration des donnÃ©es en 2 parties : entraÃ®nement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "train_X = vectorizer_G.fit_transform(train_X) \n",
    "test_X = vectorizer_G.transform(test_X)\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "modelMulti_G = MultinomialNB()\n",
    "modelMulti_G.fit(train_X, train_y)\n",
    "\n",
    "# Test the classifier\n",
    "predictions_G = modelMulti_G.predict(test_X)\n",
    "score_NVB_G = accuracy_score(test_y, predictions_G) * 100\n",
    "precision_NVB_G = precision_score(test_y, predictions_G, average = None) * 100\n",
    "recall_NVB_G = recall_score(test_y, predictions_G, average = None) * 100\n",
    "print('Accuracy du modÃ¨le sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions_G) * 100, 2), '%')\n",
    "report = classification_report(test_y, predictions_G)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.729164600Z"
    }
   },
   "id": "ef8772629236bdda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Vectorisation des critiques\n",
    "vectorizer = CountVectorizer(max_features=8000)\n",
    "\n",
    "\n",
    "# SÃ©paration des donnÃ©es en 2 parties : entraÃ®nement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=20, min_samples_split=2, min_samples_leaf=1)\n",
    "decision_tree = decision_tree.fit(train_X, train_y)\n",
    "predictions_tree = decision_tree.predict(test_X)\n",
    "score_DT = accuracy_score(test_y, predictions_tree) * 100\n",
    "precision_DT = precision_score(test_y, predictions_tree, average = None) * 100\n",
    "recall_DT = recall_score(test_y, predictions_tree, average = None) * 100\n",
    "print('Accuracy du modÃ¨le decision_tree avec CountVectorizer sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions_tree) * 100, 2), '%')\n",
    "\n",
    "report = classification_report(test_y, predictions_tree)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.777193100Z"
    }
   },
   "id": "497e8fb044699be4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Vectorisation des critiques\n",
    "vectorizer = CountVectorizer(max_features = 8000)\n",
    "\n",
    "\n",
    "# SÃ©paration des donnÃ©es en 2 parties : entraÃ®nement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "rf = RandomForestClassifier(max_depth = None, random_state = 32, n_estimators = 1000)\n",
    "rf = rf.fit(train_X, train_y)\n",
    "predictions = rf.predict(test_X)\n",
    "score_RF = accuracy_score(test_y, predictions) * 100\n",
    "precision_RF = precision_score(test_y, predictions, average = None) * 100\n",
    "recall_RF = recall_score(test_y, predictions, average = None) * 100\n",
    "print('Accuracy du modÃ¨le Random_Forest avec CountVectorizer sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions) * 100, 2), '%')\n",
    "report = classification_report(test_y, predictions)\n",
    "print(report)  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.893865Z",
     "start_time": "2024-02-18T23:19:25.799488400Z"
    }
   },
   "id": "5dc45f6ae86dc9ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "vectorizer_TFIDF = TfidfVectorizer(max_features=8000)\n",
    "\n",
    "# SÃ©paration des donnÃ©es en 2 parties : entraÃ®nement / test\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    " \n",
    "train_X = vectorizer_TFIDF.fit_transform(train_X) \n",
    "test_X = vectorizer_TFIDF.transform(test_X)\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "modelMulti_tf = MultinomialNB()\n",
    "modelMulti_tf.fit(train_X, train_y)\n",
    "\n",
    "# Test the classifier\n",
    "predictions_TF = modelMulti_tf.predict(test_X)\n",
    "score_NVB_TF = accuracy_score(test_y, predictions_TF) * 100\n",
    "precision_NVB_TF = precision_score(test_y, predictions_TF, average = None) * 100\n",
    "recall_NVB_TF= recall_score(test_y, predictions_TF, average = None) * 100\n",
    "print('Accuracy du modÃ¨le MultinomialNB avec TfidfVectorizer et sans n-grames sur l\\'ensemble de test : ', round(accuracy_score(test_y, predictions_TF) * 100, 2), '%')\n",
    "report = classification_report(test_y, predictions_TF)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.829837800Z"
    }
   },
   "id": "b3496a7947f2ae19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ModÃ¨le 01\n",
    "data.columns = [\"tweet\", \"sentiment\"]\n",
    "\n",
    "# Vectorisation des critiques\n",
    "vectorizer = CountVectorizer(max_features = 8000, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(data[\"tweet\"])\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_cl = lin_clf.fit(train_X, train_y)\n",
    "predictions = lin_cl.predict(val_X)\n",
    "score_svm = accuracy_score(val_y, predictions) * 100\n",
    "precision_svm = precision_score(val_y, predictions, average=None) * 100\n",
    "recall_svm = recall_score(val_y, predictions, average=None) * 100\n",
    "\n",
    "print('Accuracy du modÃ¨le SVM avec CountVectorizer et n-grames sur l\\'ensemble de test : ', round(score_svm, 2), '%')\n",
    "report = classification_report(val_y, predictions)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.873534900Z"
    }
   },
   "id": "4cbcb8d25d3a2fd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_data = np.array([\n",
    "    [score_NVB, precision_NVB[0], recall_NVB[0], precision_NVB[1], recall_NVB[1], precision_NVB[2], recall_NVB[2]],\n",
    "    [score_DT, precision_DT[0], recall_DT[0], precision_DT[1], recall_DT[1], precision_DT[2], recall_DT[2]],\n",
    "    [score_RF, precision_RF[0], recall_RF[0], precision_RF[1], recall_RF[1], precision_RF[2], recall_RF[2]],\n",
    "    [score_svm, precision_svm[0], recall_svm[0], precision_svm[1], recall_svm[1], precision_svm[2], recall_svm[2]]\n",
    "])\n",
    "\n",
    "# CrÃ©er un DataFrame pandas avec les colonnes appropriÃ©es\n",
    "columns = ['accuracy', 'precision_NEG', 'recall_NEG', 'precision_NEU', 'recall_NEU', 'precision_POS', 'recall_POS']\n",
    "index = ['Naive Bayes', 'Decision Tree', 'Random Forest', 'SVM']\n",
    "\n",
    "df = pd.DataFrame(df_data, columns=columns, index=index)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-18T23:19:25.887218100Z"
    }
   },
   "id": "268b187442b4a7ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_data = np.array([\n",
    "    [score_NVB, precision_NVB[0], recall_NVB[0], precision_NVB[1], recall_NVB[1], precision_NVB[2], recall_NVB[2]],\n",
    "    [score_NVB_G, precision_NVB_G[0], recall_NVB_G[0], precision_NVB_G[1], recall_NVB_G[1], precision_NVB_G[2], recall_NVB_G[2]],\n",
    "    [score_NVB_TF, precision_NVB_TF[0], recall_NVB_TF[0], precision_NVB_TF[1], recall_NVB_TF[1], precision_NVB_TF[2], recall_NVB_TF[2]],\n",
    "    [score_NVB_TF_G, precision_NVB_TF_G[0], recall_NVB_TF_G[0], precision_NVB_TF_G[1], recall_NVB_TF_G[1], precision_NVB_TF_G[2], recall_NVB_TF_G[2]]\n",
    "])\n",
    "\n",
    "# CrÃ©er un DataFrame pandas avec les colonnes appropriÃ©es\n",
    "columns = ['accuracy', 'precision_NEG', 'recall_NEG', 'precision_NEU', 'recall_NEU', 'precision_POS', 'recall_POS']\n",
    "index = ['Naive Bayes avec CountVectorizer 1_gram', 'Naive Bayes avec CountVectorizer 3_gram ',\n",
    "         'Naive Bayes avec TfidfVectorizer 1_gram', 'Naive Bayes avec TfidfVectorizer 3_gram']\n",
    "\n",
    "df = pd.DataFrame(df_data, columns=columns, index=index)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T23:19:25.905453300Z",
     "start_time": "2024-02-18T23:19:25.900383900Z"
    }
   },
   "id": "b8be824d8ded1635"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
